import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
import argparse
import torch
import numpy as np
import cv2
from torch.utils.data import DataLoader
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib import cm

from data_tools import get_transforms, create_dataset
from models import BaselineModel
from denoiser import DenoisingFCNWithSkip

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ logger å¯¹è±¡ï¼ˆç”¨äºå…¼å®¹ create_datasetï¼‰
class SimpleLogger:
    def info(self, msg):
        print(msg)

class GradCAM:
    """Grad-CAM å®ç°ç”¨äºå¯è§†åŒ–æ¨¡å‹å…³æ³¨åŒºåŸŸ"""

    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None

        # æ³¨å†Œé’©å­
        # åœ¨ __init__ æ–¹æ³•ä¸­ï¼Œå°†ç¬¬ 32-33 è¡Œæ”¹ä¸ºï¼š
        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_full_backward_hook(self.save_gradient)  # æ”¹ä¸º register_full_backward_hook

    def save_activation(self, module, input, output):
        self.activations = output.detach()

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()

    def generate_cam(self, input_image, target_class=None):
        """
        ç”Ÿæˆç±»æ¿€æ´»å›¾
        Args:
            input_image: è¾“å…¥å›¾åƒ tensor [1, C, H, W]
            target_class: ç›®æ ‡ç±»åˆ«ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨é¢„æµ‹ç±»åˆ«ï¼‰
        Returns:
            cam: çƒ­åŠ›å›¾ numpy array [H, W]
        """
        # å‰å‘ä¼ æ’­
        model_output = self.model(input_image)

        if target_class is None:
            target_class = model_output.squeeze()

        # åå‘ä¼ æ’­
        self.model.zero_grad()
        target_class.backward(retain_graph=True)

        # è·å–æ¢¯åº¦å’Œæ¿€æ´»
        gradients = self.gradients  # [1, C, H', W']
        activations = self.activations  # [1, C, H', W']

        # å…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦
        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)  # [1, C, 1, 1]

        # åŠ æƒæ±‚å’Œ
        cam = torch.sum(weights * activations, dim=1).squeeze()  # [H', W']

        # ReLU
        cam = torch.relu(cam)

        # å½’ä¸€åŒ–åˆ° [0, 1]
        cam = cam.cpu().numpy()
        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)

        return cam


def get_target_layer(model):
    for name, module in reversed(list(model.named_modules())):
        if isinstance(module, torch.nn.Conv2d):
            return module
    raise ValueError("æ²¡æœ‰æ‰¾åˆ°Conv2då±‚")


def overlay_heatmap(image, heatmap, alpha=0.5, colormap=cv2.COLORMAP_JET):
    """
    å°†çƒ­åŠ›å›¾å åŠ åˆ°åŸå›¾ä¸Š
    Args:
        image: åŸå§‹å›¾åƒ numpy array [H, W, 3] (RGB, 0-255)
        heatmap: çƒ­åŠ›å›¾ numpy array [H, W] (0-1)
        alpha: å åŠ é€æ˜åº¦
        colormap: OpenCV é¢œè‰²æ˜ å°„
    Returns:
        å åŠ åçš„å›¾åƒ [H, W, 3]
    """
    # å°†çƒ­åŠ›å›¾è°ƒæ•´åˆ°ä¸å›¾åƒç›¸åŒå¤§å°
    h, w = image.shape[:2]
    heatmap_resized = cv2.resize(heatmap, (w, h))

    # è½¬æ¢ä¸ºå½©è‰²çƒ­åŠ›å›¾
    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), colormap)
    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)

    # å åŠ 
    overlayed = cv2.addWeighted(image, 1 - alpha, heatmap_colored, alpha, 0)

    return overlayed


def denormalize_image(tensor):
    """
    åå½’ä¸€åŒ–å›¾åƒç”¨äºæ˜¾ç¤º
    Args:
        tensor: å›¾åƒ tensor [C, H, W]
        model_name: æ¨¡å‹åç§°ï¼Œç”¨äºåˆ¤æ–­å½’ä¸€åŒ–æ–¹å¼
    Returns:
        åå½’ä¸€åŒ–åçš„ tensorï¼ŒèŒƒå›´ [0, 1]
    """
    tensor = tensor.clone()
    # å…¶ä»–æ¨¡å‹ï¼ˆEfficientNetç­‰ï¼‰ä½¿ç”¨ -1~1 èŒƒå›´
    # ä» [-1, 1] è½¬æ¢åˆ° [0, 1]
    tensor = (tensor + 1.0) / 2.0
    tensor = tensor.clamp(0, 1)

    return tensor


def visualize_samples(model, dataloader, args, output_dir, noiser=None, max_samples=50):
    """
    å¯è§†åŒ–æ•°æ®é›†æ ·æœ¬çš„çƒ­åŠ›å›¾
    """
    model.eval()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_dir)
    real_dir = output_dir / "real"
    fake_dir = output_dir / "fake"
    real_dir.mkdir(parents=True, exist_ok=True)
    fake_dir.mkdir(parents=True, exist_ok=True)

    # è·å–ç›®æ ‡å±‚å¹¶åˆ›å»º Grad-CAM
    target_layer = get_target_layer(model)
    grad_cam = GradCAM(model, target_layer)

    print(f"å¼€å§‹ç”Ÿæˆçƒ­åŠ›å›¾ï¼Œæœ€å¤š {max_samples} ä¸ªæ ·æœ¬...")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    real_count = 0
    fake_count = 0

    for batch_idx, (imgs, labels, video_names, frame_indices) in enumerate(dataloader):
        if real_count >= max_samples // 2 and fake_count >= max_samples // 2:
            break

        imgs = imgs.float().to(device)
        labels = labels.to(device)

        # å¦‚æœä½¿ç”¨ noiserï¼Œå…ˆå¤„ç†å›¾åƒ
        processed_imgs = imgs.clone()
        if args.use_noiser and noiser is not None:
            res = noiser(processed_imgs)
            processed_imgs = res - res.floor()

        # é€ä¸ªæ ·æœ¬å¤„ç†
        for i in range(imgs.size(0)):
            label = labels[i].item()

            # æ ¹æ®æ ‡ç­¾å†³å®šæ˜¯å¦ç»§ç»­
            if label == 1 and real_count >= max_samples // 2:
                continue
            if label == 0 and fake_count >= max_samples // 2:
                continue

            # å‡†å¤‡å•ä¸ªæ ·æœ¬
            img_tensor = processed_imgs[i:i + 1]
            img_tensor.requires_grad = True

            # ===== ğŸ†• ç”Ÿæˆä¸¤ç§çƒ­åŠ›å›¾ =====

            # 1ï¸âƒ£ åŸºäºé¢„æµ‹çš„çƒ­åŠ›å›¾ï¼ˆä¿®å¤ç‰ˆï¼‰
            model.zero_grad()
            model_output = model(img_tensor)

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ¹æ®é¢„æµ‹ç»“æœå†³å®šæ¢¯åº¦æ–¹å‘
            prob = torch.sigmoid(model_output).item()
            pred_label = 1 if prob > 0.5 else 0

            if pred_label == 1:  # é¢„æµ‹ä¸ºReal
                target_predicted = model_output.squeeze()  # è§£é‡Š"ä¸ºä»€ä¹ˆæ˜¯Real"
            else:  # é¢„æµ‹ä¸ºFake
                target_predicted = -model_output.squeeze()  # è§£é‡Š"ä¸ºä»€ä¹ˆæ˜¯Fake"ï¼ˆå–è´Ÿï¼‰

            cam_predicted = grad_cam.generate_cam(img_tensor, target_class=target_predicted)

            # æ¸…é™¤æ¢¯åº¦
            model.zero_grad()
            img_tensor.grad = None

            # 2ï¸âƒ£ åŸºäºçœŸå®æ ‡ç­¾çš„çƒ­åŠ›å›¾ï¼ˆä¿æŒä¸å˜ï¼‰
            model_output = model(img_tensor)  # é‡æ–°å‰å‘ä¼ æ’­
            if label == 1:  # çœŸè„¸
                target_gt = model_output.squeeze()
            else:  # å‡è„¸
                target_gt = -model_output.squeeze()
            cam_gt = grad_cam.generate_cam(img_tensor, target_class=target_gt)

            # æ¸…é™¤æ¢¯åº¦
            img_tensor = img_tensor.detach()
            img_tensor.requires_grad = False
            model.zero_grad()

            # å‡†å¤‡åŸå§‹å›¾åƒç”¨äºæ˜¾ç¤º
            original_img = imgs[i].cpu()
            original_img = denormalize_image(original_img)
            original_img = (original_img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)

            # ç”Ÿæˆå åŠ å›¾
            overlay_predicted = overlay_heatmap(original_img, cam_predicted, alpha=0.5)
            overlay_gt = overlay_heatmap(original_img, cam_gt, alpha=0.5)

            # è·å–é¢„æµ‹ç»“æœ
            prob = torch.sigmoid(model_output).item()
            pred_label = 1 if prob > 0.5 else 0
            is_correct = (pred_label == label)

            # ===== ğŸ¨ åˆ›å»º5å¼ å›¾çš„å¯è§†åŒ– =====
            fig, axes = plt.subplots(1, 5, figsize=(25, 5))

            # å›¾1: åŸå›¾
            axes[0].imshow(original_img)
            axes[0].set_title('Original Image', fontsize=12, fontweight='bold')
            axes[0].axis('off')

            # å›¾2: åŸºäºé¢„æµ‹çš„çƒ­åŠ›å›¾
            im1 = axes[1].imshow(cam_predicted, cmap='jet')
            true_label_text = "Real" if label == 1 else "Fake"
            pred_label_text = "Real" if pred_label == 1 else "Fake"
            axes[1].set_title(
                f'Heatmap (Prediction)\n'
                f'Model predicts: {pred_label_text}\n'
                f'Confidence: {prob:.3f}',
                fontsize=11
            )
            axes[1].axis('off')
            plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)

            # å›¾3: åŸºäºé¢„æµ‹çš„å åŠ å›¾
            axes[2].imshow(overlay_predicted)
            axes[2].set_title(
                f'Overlay (Prediction)\n'
                f'What model actually sees',
                fontsize=11
            )
            axes[2].axis('off')

            # å›¾4: åŸºäºçœŸå®æ ‡ç­¾çš„çƒ­åŠ›å›¾
            im2 = axes[3].imshow(cam_gt, cmap='jet')
            axes[3].set_title(
                f'Heatmap (Ground Truth)\n'
                f'True label: {true_label_text}\n'
                f'Should focus here',
                fontsize=11
            )
            axes[3].axis('off')
            plt.colorbar(im2, ax=axes[3], fraction=0.046, pad=0.04)

            # å›¾5: åŸºäºçœŸå®æ ‡ç­¾çš„å åŠ å›¾
            axes[4].imshow(overlay_gt)
            correctness = "âœ“ Correct" if is_correct else "âœ— Wrong"
            axes[4].set_title(
                f'Overlay (Ground Truth)\n'
                f'True: {true_label_text}, Pred: {pred_label_text}\n'
                f'{correctness}',
                fontsize=11,
                color='green' if is_correct else 'red',
                fontweight='bold'
            )
            axes[4].axis('off')

            plt.tight_layout()

            # ä¿å­˜
            # æ¸…ç† video_name
            if isinstance(video_names[i], str):
                video_name = Path(video_names[i]).stem
                video_name = video_name.replace('\\', '_').replace('/', '_').replace(':', '_')
            else:
                video_name = f"video_{batch_idx}_{i}"

            # æ¸…ç† frame_idx
            if hasattr(frame_indices[i], 'item'):
                frame_idx = frame_indices[i].item()
            else:
                frame_idx = frame_indices[i]

            if isinstance(frame_idx, str):
                frame_idx_clean = Path(frame_idx).stem
                frame_idx_clean = frame_idx_clean.replace('\\', '_').replace('/', '_').replace(':', '_')
            else:
                frame_idx_clean = str(frame_idx)

            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼Œæ·»åŠ æ­£ç¡®æ€§æ ‡è®°
            correctness_tag = "correct" if is_correct else "wrong"
            filename = f"{video_name}_frame{frame_idx_clean}_{correctness_tag}_prob{prob:.3f}.png"

            if label == 1:
                save_path = real_dir / filename
                real_count += 1
            else:
                save_path = fake_dir / filename
                fake_count += 1

            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()

            print(f"å·²ä¿å­˜: {save_path.name} (Real: {real_count}, Fake: {fake_count})")

            if real_count >= max_samples // 2 and fake_count >= max_samples // 2:
                break

    print(f"\nå®Œæˆ! å…±ç”Ÿæˆ {real_count + fake_count} ä¸ªçƒ­åŠ›å›¾")
    print(f"Real æ ·æœ¬: {real_count}")
    print(f"Fake æ ·æœ¬: {fake_count}")


def get_parser():
    parser = argparse.ArgumentParser(description="çƒ­åŠ›å›¾å¯è§†åŒ–è„šæœ¬")

    # æ¨¡å‹ç›¸å…³
    parser.add_argument("--checkpoint", type=str, default=r'C:\Users\jumpycat\OneDrive\03-Projects\30 - DETECTOR\v3\runs\20260213104013_efficientnet_b0\checkpoints\ckpt_step_0050000.pt', help="æ¨¡å‹ checkpoint è·¯å¾„")
    parser.add_argument("--model_name", type=str, default="efficientnet_b0", help="æ¨¡å‹åç§°")
    parser.add_argument("--full_resize", action="store_false", help="æ˜¯å¦åœ¨ AE å¢å¹¿ä¸­å¯ç”¨ Patch Blending (æ··åˆæŒ‡çº¹)")

    # æ•°æ®ç›¸å…³ # E:\00_fjw\00_data\FF++\FaceForensics++_RAW_split_cropped\test  E:\00_fjw\00_data\DeepSpeak\deepspeak_exported\train_cropped_faces
    parser.add_argument("--data_root", type=str, default=r'E:\00_fjw\00_data\DeepSpeak\deepspeak_exported\train_cropped_faces', help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--img_size", type=int, default=224)
    parser.add_argument("--max_videos", type=int, default=32, help="æ¯ä¸ªç±»åˆ«æœ€å¤§è§†é¢‘æ•°")
    parser.add_argument("--max_frames", type=int, default=4, help="æ¯ä¸ªè§†é¢‘æœ€å¤§å¸§æ•°")
    parser.add_argument("--fake_types", nargs='+', default=['Deepfakes', 'Face2Face', 'FaceSwap'], help="Fake ç±»å‹ï¼ˆç”¨äº FF++ æ•°æ®é›†ï¼‰")
    parser.add_argument("--ff_split", type=str, default='test', choices=['train', 'val', 'test'])

    # Noiser ç›¸å…³
    parser.add_argument("--use_noiser", action="store_true", help="æ˜¯å¦ä½¿ç”¨ noiser")
    parser.add_argument("--noiser_resume", type=str, default=None, help="Noiser checkpoint è·¯å¾„")

    # å¯è§†åŒ–ç›¸å…³
    parser.add_argument("--output_dir", type=str, default=r"C:\Users\jumpycat\OneDrive\03-Projects\30 - DETECTOR\v3\runs\20260213104013_efficientnet_b0\heatmap_visualizations", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max_samples", type=int, default=128, help="æœ€å¤§å¯è§†åŒ–æ ·æœ¬æ•°ï¼ˆçœŸä¼ªå„å ä¸€åŠï¼‰")
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--num_workers", type=int, default=8)

    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42)

    return parser


def main(args):
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {args.checkpoint}")
    model = BaselineModel(args=args).to(device)

    checkpoint = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(checkpoint["model_state"])
    model.eval()

    print("æ¨¡å‹åŠ è½½æˆåŠŸ!")

    # 2. åŠ è½½ Noiserï¼ˆå¦‚æœéœ€è¦ï¼‰
    noiser = None
    if args.use_noiser:
        if args.noiser_resume and os.path.exists(args.noiser_resume):
            print(f"æ­£åœ¨åŠ è½½ Noiser: {args.noiser_resume}")
            noiser = DenoisingFCNWithSkip().to(device)
            noiser_state = torch.load(args.noiser_resume, map_location=device)["noiser"]
            noiser.load_state_dict(noiser_state, strict=False)
            noiser.eval()
            for p in noiser.parameters():
                p.requires_grad = False
            print("Noiser åŠ è½½æˆåŠŸ!")

    # 3. å‡†å¤‡æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    transform, normalize = get_transforms(args)

    logger = SimpleLogger()

    dataset = create_dataset(
        data_root=args.data_root,
        max_videos=args.max_videos,
        max_frames=args.max_frames,
        args=args,
        logger=logger,
        transform=transform,
        normalize=normalize,
        augmentation=None  # å¯è§†åŒ–æ—¶ä¸éœ€è¦å¢å¹¿
    )

    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True
    )

    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

    # 4. ç”Ÿæˆå¯è§†åŒ–
    visualize_samples(
        model=model,
        dataloader=dataloader,
        args=args,
        output_dir=args.output_dir,
        noiser=noiser,
        max_samples=args.max_samples
    )

    print("\nå…¨éƒ¨å®Œæˆ!")


if __name__ == "__main__":
    parser = get_parser()
    args = parser.parse_args()
    main(args)